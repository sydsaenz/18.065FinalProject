{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9295c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d50a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('team_features.csv')\n",
    "matches = pd.read_csv('MNCAATourneyCompactResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12c0ec7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Season",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "WTeamID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "LTeamID",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bcd1c5f5-70aa-4434-bf77-5a69b3fdd833",
       "rows": [
        [
         "0",
         "1985",
         "1116",
         "1234"
        ],
        [
         "1",
         "1985",
         "1120",
         "1345"
        ],
        [
         "2",
         "1985",
         "1207",
         "1250"
        ],
        [
         "3",
         "1985",
         "1229",
         "1425"
        ],
        [
         "4",
         "1985",
         "1242",
         "1325"
        ],
        [
         "5",
         "1985",
         "1246",
         "1449"
        ],
        [
         "6",
         "1985",
         "1256",
         "1338"
        ],
        [
         "7",
         "1985",
         "1260",
         "1233"
        ],
        [
         "8",
         "1985",
         "1314",
         "1292"
        ],
        [
         "9",
         "1985",
         "1323",
         "1333"
        ],
        [
         "10",
         "1985",
         "1326",
         "1235"
        ],
        [
         "11",
         "1985",
         "1328",
         "1299"
        ],
        [
         "12",
         "1985",
         "1374",
         "1330"
        ],
        [
         "13",
         "1985",
         "1385",
         "1380"
        ],
        [
         "14",
         "1985",
         "1396",
         "1439"
        ],
        [
         "15",
         "1985",
         "1424",
         "1361"
        ],
        [
         "16",
         "1985",
         "1104",
         "1112"
        ],
        [
         "17",
         "1985",
         "1130",
         "1403"
        ],
        [
         "18",
         "1985",
         "1181",
         "1337"
        ],
        [
         "19",
         "1985",
         "1208",
         "1455"
        ],
        [
         "20",
         "1985",
         "1210",
         "1273"
        ],
        [
         "21",
         "1985",
         "1228",
         "1318"
        ],
        [
         "22",
         "1985",
         "1268",
         "1275"
        ],
        [
         "23",
         "1985",
         "1272",
         "1335"
        ],
        [
         "24",
         "1985",
         "1276",
         "1192"
        ],
        [
         "25",
         "1985",
         "1298",
         "1261"
        ],
        [
         "26",
         "1985",
         "1301",
         "1305"
        ],
        [
         "27",
         "1985",
         "1393",
         "1177"
        ],
        [
         "28",
         "1985",
         "1412",
         "1277"
        ],
        [
         "29",
         "1985",
         "1431",
         "1409"
        ],
        [
         "30",
         "1985",
         "1433",
         "1267"
        ],
        [
         "31",
         "1985",
         "1437",
         "1173"
        ],
        [
         "32",
         "1985",
         "1120",
         "1242"
        ],
        [
         "33",
         "1985",
         "1207",
         "1396"
        ],
        [
         "34",
         "1985",
         "1246",
         "1424"
        ],
        [
         "35",
         "1985",
         "1256",
         "1326"
        ],
        [
         "36",
         "1985",
         "1260",
         "1374"
        ],
        [
         "37",
         "1985",
         "1314",
         "1323"
        ],
        [
         "38",
         "1985",
         "1328",
         "1229"
        ],
        [
         "39",
         "1985",
         "1385",
         "1116"
        ],
        [
         "40",
         "1985",
         "1104",
         "1433"
        ],
        [
         "41",
         "1985",
         "1130",
         "1181"
        ],
        [
         "42",
         "1985",
         "1210",
         "1393"
        ],
        [
         "43",
         "1985",
         "1228",
         "1208"
        ],
        [
         "44",
         "1985",
         "1268",
         "1298"
        ],
        [
         "45",
         "1985",
         "1272",
         "1412"
        ],
        [
         "46",
         "1985",
         "1301",
         "1431"
        ],
        [
         "47",
         "1985",
         "1437",
         "1276"
        ],
        [
         "48",
         "1985",
         "1207",
         "1260"
        ],
        [
         "49",
         "1985",
         "1210",
         "1228"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2518
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>WTeamID</th>\n",
       "      <th>LTeamID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>1116</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985</td>\n",
       "      <td>1120</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985</td>\n",
       "      <td>1207</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985</td>\n",
       "      <td>1229</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>1242</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2024</td>\n",
       "      <td>1301</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2024</td>\n",
       "      <td>1345</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2024</td>\n",
       "      <td>1163</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>2024</td>\n",
       "      <td>1345</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>2024</td>\n",
       "      <td>1163</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2518 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season  WTeamID  LTeamID\n",
       "0       1985     1116     1234\n",
       "1       1985     1120     1345\n",
       "2       1985     1207     1250\n",
       "3       1985     1229     1425\n",
       "4       1985     1242     1325\n",
       "...      ...      ...      ...\n",
       "2513    2024     1301     1181\n",
       "2514    2024     1345     1397\n",
       "2515    2024     1163     1104\n",
       "2516    2024     1345     1301\n",
       "2517    2024     1163     1345\n",
       "\n",
       "[2518 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.drop(columns = ['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8298caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Season', 'DayNum', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'WLoc',\n",
      "       'NumOT'],\n",
      "      dtype='object')\n",
      "Index(['index', 'TeamID', 'Season', 'seed_num', 'weighted_past_seed',\n",
      "       '3pt_success_rate', 'freethrow_success_rate', 'field_success_rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(matches.columns)\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd77e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.merge(matches[['Season', 'WTeamID', 'LTeamID']], features[['TeamID', 'Season', 'weighted_past_seed',\n",
    "       '3pt_success_rate', 'freethrow_success_rate', 'field_success_rate']], left_on=['Season', 'WTeamID'], right_on = ['Season', 'TeamID'])\n",
    "\n",
    "matches = matches.drop(columns = ['TeamID'])\n",
    "matches = matches.rename(columns = {'weighted_past_seed': 'weighted_past_seed_W', \n",
    "                                    '3pt_success_rate': '3pt_success_rate_W', \n",
    "                                    'freethrow_success_rate': 'freethrow_success_rate_W',\n",
    "                                    'freethrow_success_rate': 'freethrow_success_rate_W',\n",
    "                                    'field_success_rate': 'field_success_rate_W'\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e5860002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'WTeamID', 'LTeamID', 'weighted_past_seed_W',\n",
       "       '3pt_success_rate_W', 'freethrow_success_rate_W',\n",
       "       'field_success_rate_W'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d957390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.merge(matches[['Season', 'WTeamID', 'LTeamID', 'weighted_past_seed_W',\n",
    "       '3pt_success_rate_W', 'freethrow_success_rate_W',\n",
    "       'field_success_rate_W']], features[['TeamID', 'Season', 'weighted_past_seed',\n",
    "       '3pt_success_rate', 'freethrow_success_rate', 'field_success_rate']], left_on=['Season', 'LTeamID'], right_on = ['Season', 'TeamID'])\n",
    "\n",
    "matches = matches.drop(columns=['TeamID'])\n",
    "matches = matches.rename(columns = {'weighted_past_seed': 'weighted_past_seed_L', \n",
    "                                    '3pt_success_rate': '3pt_success_rate_L', \n",
    "                                    'freethrow_success_rate': 'freethrow_success_rate_L',\n",
    "                                    'field_success_rate': 'field_success_rate_L'\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5234bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = {'weighted_past_seed', '3pt_success_rate', 'freethrow_success_rate', 'field_success_rate'}\n",
    "for col in all_cols:\n",
    "    matches[col+'_diff'] = matches[col+'_W'] - matches[col+'_L']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e3647843",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches[[col+'_diff' for col in all_cols]].copy()  #must do copy to create a new df, not a slice\n",
    "matches_negated = matches.copy()\n",
    "for col in all_cols:\n",
    "    matches_negated[col+'_diff'] = - matches[col+'_diff']\n",
    "\n",
    "matches['label'] = np.full(len(matches), 1) #diff features have true label 1\n",
    "matches_negated['label'] = np.full(len(matches_negated), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a11a5b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['weighted_past_seed_diff', '3pt_success_rate_diff',\n",
       "       'freethrow_success_rate_diff', 'field_success_rate_diff', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([matches, matches_negated])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 77.4075\n",
      "Epoch 2/100, Loss: 74.4650\n",
      "Epoch 3/100, Loss: 74.3069\n",
      "Epoch 4/100, Loss: 74.0622\n",
      "Epoch 5/100, Loss: 74.0348\n",
      "Epoch 6/100, Loss: 73.9467\n",
      "Epoch 7/100, Loss: 73.8816\n",
      "Epoch 8/100, Loss: 73.9873\n",
      "Epoch 9/100, Loss: 73.8842\n",
      "Epoch 10/100, Loss: 73.8214\n",
      "Epoch 11/100, Loss: 73.7184\n",
      "Epoch 12/100, Loss: 73.6407\n",
      "Epoch 13/100, Loss: 73.6477\n",
      "Epoch 14/100, Loss: 73.5365\n",
      "Epoch 15/100, Loss: 73.6837\n",
      "Epoch 16/100, Loss: 73.5668\n",
      "Epoch 17/100, Loss: 73.5495\n",
      "Epoch 18/100, Loss: 73.4951\n",
      "Epoch 19/100, Loss: 73.4366\n",
      "Epoch 20/100, Loss: 73.3543\n",
      "Epoch 21/100, Loss: 73.3790\n",
      "Epoch 22/100, Loss: 73.3572\n",
      "Epoch 23/100, Loss: 73.2105\n",
      "Epoch 24/100, Loss: 73.3361\n",
      "Epoch 25/100, Loss: 73.1483\n",
      "Epoch 26/100, Loss: 73.2563\n",
      "Epoch 27/100, Loss: 73.2565\n",
      "Epoch 28/100, Loss: 73.0386\n",
      "Epoch 29/100, Loss: 73.0546\n",
      "Epoch 30/100, Loss: 72.8909\n",
      "Epoch 31/100, Loss: 73.0344\n",
      "Epoch 32/100, Loss: 72.9648\n",
      "Epoch 33/100, Loss: 72.9608\n",
      "Epoch 34/100, Loss: 72.9499\n",
      "Epoch 35/100, Loss: 73.0846\n",
      "Epoch 36/100, Loss: 72.7797\n",
      "Epoch 37/100, Loss: 72.7860\n",
      "Epoch 38/100, Loss: 72.7435\n",
      "Epoch 39/100, Loss: 72.7523\n",
      "Epoch 40/100, Loss: 72.8570\n",
      "Epoch 41/100, Loss: 72.6742\n",
      "Epoch 42/100, Loss: 72.7325\n",
      "Epoch 43/100, Loss: 72.6697\n",
      "Epoch 44/100, Loss: 72.6355\n",
      "Epoch 45/100, Loss: 72.5689\n",
      "Epoch 46/100, Loss: 72.5611\n",
      "Epoch 47/100, Loss: 72.3816\n",
      "Epoch 48/100, Loss: 72.5226\n",
      "Epoch 49/100, Loss: 72.4291\n",
      "Epoch 50/100, Loss: 72.3385\n",
      "Epoch 51/100, Loss: 72.2539\n",
      "Epoch 52/100, Loss: 72.4589\n",
      "Epoch 53/100, Loss: 72.2119\n",
      "Epoch 54/100, Loss: 72.2622\n",
      "Epoch 55/100, Loss: 72.1406\n",
      "Epoch 56/100, Loss: 72.1598\n",
      "Epoch 57/100, Loss: 72.0701\n",
      "Epoch 58/100, Loss: 72.0517\n",
      "Epoch 59/100, Loss: 72.0845\n",
      "Epoch 60/100, Loss: 72.1277\n",
      "Epoch 61/100, Loss: 72.0783\n",
      "Epoch 62/100, Loss: 71.8403\n",
      "Epoch 63/100, Loss: 71.9228\n",
      "Epoch 64/100, Loss: 71.9900\n",
      "Epoch 65/100, Loss: 71.7879\n",
      "Epoch 66/100, Loss: 71.8718\n",
      "Epoch 67/100, Loss: 71.7845\n",
      "Epoch 68/100, Loss: 71.6180\n",
      "Epoch 69/100, Loss: 71.6821\n",
      "Epoch 70/100, Loss: 71.7751\n",
      "Epoch 71/100, Loss: 71.5715\n",
      "Epoch 72/100, Loss: 71.6385\n",
      "Epoch 73/100, Loss: 71.5951\n",
      "Epoch 74/100, Loss: 71.5423\n",
      "Epoch 75/100, Loss: 71.6682\n",
      "Epoch 76/100, Loss: 71.5846\n",
      "Epoch 77/100, Loss: 71.4193\n",
      "Epoch 78/100, Loss: 71.4928\n",
      "Epoch 79/100, Loss: 71.5340\n",
      "Epoch 80/100, Loss: 71.3188\n",
      "Epoch 81/100, Loss: 71.1942\n",
      "Epoch 82/100, Loss: 71.1901\n",
      "Epoch 83/100, Loss: 71.2468\n",
      "Epoch 84/100, Loss: 71.1590\n",
      "Epoch 85/100, Loss: 71.2287\n",
      "Epoch 86/100, Loss: 71.0531\n",
      "Epoch 87/100, Loss: 71.0954\n",
      "Epoch 88/100, Loss: 70.9425\n",
      "Epoch 89/100, Loss: 70.8768\n",
      "Epoch 90/100, Loss: 71.0691\n",
      "Epoch 91/100, Loss: 71.0216\n",
      "Epoch 92/100, Loss: 71.0308\n",
      "Epoch 93/100, Loss: 70.8397\n",
      "Epoch 94/100, Loss: 70.9540\n",
      "Epoch 95/100, Loss: 70.8057\n",
      "Epoch 96/100, Loss: 70.6679\n",
      "Epoch 97/100, Loss: 70.6584\n",
      "Epoch 98/100, Loss: 70.7459\n",
      "Epoch 99/100, Loss: 70.7158\n",
      "Epoch 100/100, Loss: 70.7100\n",
      "Test Accuracy: 0.6796\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import pandas as pd\n",
    "\n",
    "# # Concatenate and prepare the data\n",
    "# X = data.drop(columns='label').values.astype('float32')\n",
    "# Y = data['label'].values.astype('float32')\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# # Split the dataset\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train)\n",
    "# Y_train_tensor = torch.tensor(Y_train).unsqueeze(1)\n",
    "# X_test_tensor = torch.tensor(X_test)\n",
    "# Y_test_tensor = torch.tensor(Y_test).unsqueeze(1)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Define a simple neural network\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = Classifier(input_dim=X.shape[1])\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# epochs = 100\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     for X_batch, Y_batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(X_batch)\n",
    "#         loss = criterion(output, Y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# # Evaluation on test data\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor)\n",
    "#     predicted_labels = (predictions >= 0.5).float()\n",
    "#     accuracy = (predicted_labels == Y_test_tensor).float().mean()\n",
    "#     print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "004ae0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = data.drop(columns='label').values.astype('float32')\n",
    "Y = data['label'].values.astype('float32')\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "671c391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train)\n",
    "X_val_tensor = torch.tensor(X_val)\n",
    "Y_train_tensor = torch.tensor(Y_train).unsqueeze(1)\n",
    "Y_val_tensor = torch.tensor(Y_val).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0998c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid())       #nn.Sequential takes in batchsize x input_dim\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b090a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNN(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "#learning rate too high will cause exploding weights -> NaNs in y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7acaafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: Loss  0.5882\n",
      "epoch 20: Loss  0.5854\n",
      "epoch 30: Loss  0.5850\n",
      "epoch 40: Loss  0.5840\n",
      "epoch 50: Loss  0.5839\n",
      "epoch 60: Loss  0.5828\n",
      "epoch 70: Loss  0.5827\n",
      "epoch 80: Loss  0.5828\n",
      "epoch 90: Loss  0.5820\n",
      "epoch 100: Loss  0.5816\n",
      "epoch 110: Loss  0.5809\n",
      "epoch 120: Loss  0.5806\n",
      "epoch 130: Loss  0.5803\n",
      "epoch 140: Loss  0.5806\n",
      "epoch 150: Loss  0.5784\n",
      "epoch 160: Loss  0.5788\n",
      "epoch 170: Loss  0.5789\n",
      "epoch 180: Loss  0.5774\n",
      "epoch 190: Loss  0.5779\n",
      "epoch 200: Loss  0.5766\n",
      "validation accuracy:  68.25%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "max_epoch = 200\n",
    "for epoch in range(max_epoch):\n",
    "    avg_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        avg_loss += loss.item()/len(train_loader)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}: Loss {avg_loss: .4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in val_loader:\n",
    "        y_pred = model(x)\n",
    "        pred = y_pred >= 0.5\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += len(x)\n",
    "print(f\"validation accuracy: {correct/total: .2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c0502283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
